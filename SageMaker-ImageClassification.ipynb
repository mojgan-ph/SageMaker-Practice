{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import boto3\n",
    "bucket_name='my_bucket_name'\n",
    "data_set='jpeg/train'\n",
    "region = boto3.Session().region_name\n",
    "# s3train_path = 'https://s3-{}.amazonaws.com/{}/{}'.format(region,bucket_name, data_set)\n",
    "s3train_path='s3://{}/jpeg/train/'.format(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "role= get_execution_role()\n",
    "sess=sagemaker.Session()\n",
    "training_image=get_image_uri(sess.boto_region_name, 'image-classification', repo_version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location='s3://{}/{}/output'.format(bucket_name, 'jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended input format for the Amazon SageMaker image classification algorithms is Apache MXNet RecordIO. With that format, the training process uses the data in pipe mode rather than file mode. Pipe mode is faster because it makes an stream of data instead of copying batches of data one by one. In pipe mode the training job does not have to wait for the data to arrive before starting the process on each bach. However, we can also have jpeg or png files and make it be used in pipe by using an augmented manifest file. Find more information here: https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make augmented manifest file for train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='path_to_train.csv'\n",
    "df=pd.read_csv(path)\n",
    "df.loc[:, 'file_name']= df.loc[:,'image_name']+'.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make train and validation sets\n",
    "from sklearn.utils import shuffle\n",
    "df=shuffle(df)\n",
    "val_size=int(df.shape[0]/10)\n",
    "df_val=df[0:val_size]\n",
    "df=df[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate mallignant cases for 55 times to make the data balanced!\n",
    "df_mallignant=df[df['target']==1]\n",
    "for i in range(55):\n",
    "    df=df.append(df_mallignant)\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make augmented manifest file for train dataset\n",
    "df_manifest= df[['file_name', 'target']]\n",
    "df_manifest['file_name']= 's3://{}/jpeg/train/'.format(bucket_name)+df_manifest['file_name']\n",
    "df_manifest.columns=['source-ref', 'class']\n",
    "out=json.dumps(json.loads(df_manifest.to_json(orient='records')))[1:-1].replace('}, {', \"}\\n{\")\n",
    "with open('../data/train_manifest', 'w') as f:\n",
    "    f.write(out)\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file('../data/train_manifest', Bucket=bucket_name, Key='jpeg/train_manifest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make augmented manifest file for validation dataset\n",
    "df_val_manifest= df_val[['file_name', 'target']]\n",
    "df_val_manifest['file_name']= 's3://{}/jpeg/train/'.format(bucket_name)+df_val_manifest['file_name']\n",
    "df_val_manifest.columns=['source-ref', 'class']\n",
    "out=json.dumps(json.loads(df_val_manifest.to_json(orient='records')))[1:-1].replace('}, {', \"}\\n{\")\n",
    "with open('../data/validation_manifest', 'w') as f:\n",
    "    f.write(out)\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file('../data/validation_manifest', Bucket=bucket_name, Key='jpeg/validation_manifest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train in pipe mode with augmented manifest files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object set to using \"Pipe\" mode.\n",
    "model = sagemaker.estimator.Estimator(training_image,\n",
    "                                      role,\n",
    "                                      train_instance_count=1,\n",
    "                                      train_instance_type='ml.p3.2xlarge', \n",
    "                                      train_volume_size = 20, \n",
    "                                      train_max_run = 3600*5,\n",
    "                                      input_mode = 'Pipe',\n",
    "                                      output_path=s3_output_location,\n",
    "                                      sagemaker_session=sess)\n",
    "#                                     model_uri= 's3-address-of-an-existing-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_hyperparameters=dict(\n",
    "    use_pretrained_model=1,\n",
    "    image_shape='3,224,224',\n",
    "    num_classes=2,\n",
    "    num_training_samples=58084,\n",
    ")\n",
    "\n",
    "hyperparameters={\n",
    "    **base_hyperparameters,\n",
    "    **dict(\n",
    "        learning_rate=0.001,\n",
    "        mini_batch_size=32, \n",
    "        num_layers= 101,\n",
    "        epochs=4,\n",
    "        top_k=2\n",
    "    )\n",
    "}\n",
    "\n",
    "model.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train data channel with S3_data_type as 'AugmentedManifestFile' and attribute names.\n",
    "train_data = sagemaker.session.s3_input('s3://{}/jpeg/train_manifest'.format(bucket_name),\n",
    "    distribution='FullyReplicated',\n",
    "    content_type='application/x-recordio',\n",
    "    s3_data_type='AugmentedManifestFile', # if recordIO files were use, it should have been set to's3Prefix'\n",
    "    attribute_names=['source-ref', 'class'],\n",
    "    input_mode='Pipe',\n",
    "    record_wrapping='RecordIO') \n",
    "\n",
    "validation_data=sagemaker.session.s3_input('s3://{}/jpeg/validation_manifest'.format(bucket_name),\n",
    "    distribution='FullyReplicated',\n",
    "    content_type='application/x-recordio',\n",
    "    s3_data_type='AugmentedManifestFile', \n",
    "    attribute_names=['source-ref', 'class'],\n",
    "    input_mode='Pipe',\n",
    "    record_wrapping='RecordIO') \n",
    "\n",
    "data_channels = {'train': train_data, 'validation':validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs=data_channels, job_name='my_training_job_name', logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job=model.latest_training_job\n",
    "job_name=job.name\n",
    "print(f\"available for download at: {model.output_path}/{job_name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_addr='s3://{}/jpeg/output/my_training_job_name/output/model.tar.gz'.format(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    model_data=model_addr, \n",
    "    image=training_image,\n",
    "    role=role, \n",
    "    sagemaker_session=sess \n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location of the test dataset\n",
    "batch_input = 's3://{}/jpeg/test'.format(bucket_name)\n",
    "\n",
    "# The location to store the results of the batch transform job\n",
    "batch_output = '{}/inference_output'.format(s3_output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#I tried doing batch inference in pipe mode (by provding manifest files), but it does not work. \n",
    "#Does not raise an error, and does not produce the output files\n",
    "#so, here I am doing it in file mode\n",
    "transformer = model.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)\n",
    "transformer.transform(data=batch_input, data_type='S3Prefix', content_type='image/jpeg', split_type=None)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how I tied doing the batch inference in pipe mode\n",
    "test_manifest='s3://{}/jpeg/test_manifest'.format(bucket_name)\n",
    "transformer= model.transformer(instance_count=1, instance_type='ml.m4.4xlarge', assemble_with='Line',\n",
    "                               output_path=s3_output_location)transformer.transform(data= test_manifest , \n",
    "    data_type='ManifestFile', \n",
    "    job_name='melanoma-first-transform-job-7',#if I do not give a name, it will do so automatically with date and time\n",
    "    content_type='application/x-recordio',\n",
    "    split_type='Line')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make submission file on cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import os, boto3\n",
    "import json\n",
    "\n",
    "output = \"image_name,target\\n\"\n",
    "client = boto3.client('s3')\n",
    "paginator = client.get_paginator('list_objects_v2')\n",
    "page_iterator = paginator.paginate(Bucket='mykagglebuckettobeusedbyfastai', Prefix ='jpeg/outputbatch_inference_34_6epochs/ISIC_')\n",
    "for page in page_iterator:\n",
    "    for content in page['Contents']:\n",
    "        client.download_file(Bucket='mykagglebuckettobeusedbyfastai', Key=content['Key'], Filename= 'tempFile')\n",
    "        with open(str('tempFile')) as f:\n",
    "            data = json.load(f)\n",
    "        value = round(data['prediction'][1], 4)\n",
    "        output = output + os.path.splitext(os.path.splitext(content['Key'][len('jpeg/output/batch_inference_34_6epochs'):])[0])[0] + \",\"+str(value) + \"\\n\"\n",
    "with open('submission.csv', 'w') as sub:\n",
    "    sub.write(output)\n",
    "client.upload_file('submission.csv', 'mykagglebuckettobeusedbyfastai', 'jpeg/submissions/submission_34_6epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make submission file on my local computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just keeping this code for reference\n",
    "\n",
    "Before trying to make the submission file on cloud, I downloaded the whole folder (made a zip file) that contains the output of batch transform to my own computer. Then I made the submission file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "import json\n",
    "\n",
    "output = \"image_name,target\\n\"\n",
    "for file in os.listdir(path/'output_folder'):\n",
    "    jpg_name = os.path.splitext(os.path.splitext(file)[0])[0]\n",
    "    with open(str(path/'output_folder')+\"/\"+file) as f:\n",
    "        data = json.load(f)\n",
    "        value = round(data['prediction'][1], 4)\n",
    "        output = output + jpg_name + \",\"+str(value) + \"\\n\"\n",
    "\n",
    "with open(path/'my_submission.csv', 'w') as sub:\n",
    "    sub.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "augmentation_type : crop:Randomlycroptheimageandfliptheimagehorizontally\n",
    "\n",
    "image_shape : string. Typical image dimensions for image classification are '3, 224, 224'. This is similar to the ImageNet dataset.\n",
    "\n",
    "lr_scheduler_factor : float. The ratio to reduce learning rate used in conjunction with the lr_scheduler_step parameter, defined as lr_new = lr_old * lr_scheduler_factor.\n",
    "\n",
    "lr_scheduler_step : string. The epochs at which to reduce the learning rate. As explained in the lr_scheduler_factor parameter, the learning rate is reduced by lr_scheduler_factor at these epochs. For example, if the value is set to \"10, 20\", then the learning rate is reduced by lr_scheduler_factor after 10th epoch and again by lr_scheduler_factor after 20th epoch. The epochs are delimited by \",\".\n",
    "\n",
    "momentum : float. The momentum for sgd and nag, ignored for other optimizers.\n",
    "\n",
    "num_layers : Number of layers for the network. For data with large image size (for example, 224x224 - like ImageNet), we suggest selecting the number of layers from the set [18, 34, 50, 101, 152, 200]. For data with small image size (for example, 28x28 - like CIFAR), we suggest selecting the number of layers from the set [20, 32, 44, 56, 110]. The number of layers in each set is based on the ResNet paper. For transfer learning, the number of layers defines the architecture of base network and hence can only be selected from the set [18, 34, 50, 101, 152, 200].\n",
    "\n",
    "optimizer : default sgd, adam, rmsprop, nag\n",
    "\n",
    "precision_dtype : Default value: float32. The precision of the weights used for training. The algorithm can use either single precision (float32) or half precision (float16) for the weights. Using half-precision for weights results in reduced memory consumption. Valid values: float32 or float16\n",
    "\n",
    "resize : positive integer. Resizes the image before using it for training. The images are resized so that the shortest side has the number of pixels specified by this parameter. If the parameter is not set, then the training data is used without resizing.\n",
    "\n",
    "use_weighted_loss : Flag to use weighted cross-entropy loss for multi-label classification (used only when multi_label = 1), where the weights are calculated based on the distribution of classes.\n",
    "\n",
    "use_pretrained_model : Valid values: 0 or 1\n",
    "\n",
    "mini_batch_size : integer, default 32\n",
    "\n",
    "learning_rate : default 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to look into when the need arise\n",
    "\n",
    "### how to do image normalization with sagemaker image classification?\n",
    "Fast ai does some image preprocessing (resize and transforms) and normalizatio is one of them. I cannot find how to do normalization in SageMaker Image classification there.\n",
    "\n",
    "I got AUCROC of 0.85 with resnet34 and only 2 epochs when I used fastai. With sagemaker, I got 0.60. I managed to improve it to 0.81 by doing some preprocessing: \n",
    "\n",
    "fastai does some preprocessing, like random horizontal flip or crop centre, and it does the resizing to 224*224\n",
    "These can be done in sagemaker by\n",
    "```\n",
    "        augmentation_type= 'crop',\n",
    "        resize= 224\n",
    "```\n",
    "in hyperparameters but these are done by cpu. So cpu usage goes up (close to maximum) and GPU ussage stays low at 15-20%\n",
    "\n",
    "### GPU usage\n",
    "When doing preprocesisng, my CPU ussage is close to maximum and my GPU ussage is minimal. That is because the preprocessing tasks are done by CPU. I do not know what we can do about it. Things like resize might be good to be done once, and saved on the data. But random crop and horizontal flip I believe should be done on each epoch separately. \n",
    "\n",
    "For fastai I used file mode for transferring the data from s3 for training process. I read that pipe mode is faster because the gpu will not have to wait for files to be downloaded like file mode. instead, we will have a stream of data that makes the data available for gpu before it needs it. So I used pipe mode when I started with sagemaker image classification. \n",
    "\n",
    "I think it did speed up the process, compared to the time I that fastai needed, but since fastai did normalization as well, I cannot be sure. And I used a smaller batch size with fastai.\n",
    "\n",
    "Nevertheless, the PUG ussage does not go over 40%, even if I do no preprocessing. I assume the GPU is still waiting for the data to arrive (even in pipe mode). I just read that if the data is not too big, such that it can fit in CPU memory, then it is better to use file mode, because it downloads the files once, and uses them for several epochs, while in pipe mode the data is transfered from S3 for every epoch. I checked the memory sizes and it looks like there is enough space for my training data if I use a ml.p3.2xlarge instance as it has 61G of memory. I am still not sure if I have to do something to let the algorithm know that I want it be kept in memory or it will do so automatically.\n",
    "\n",
    "### one annoying issue:\n",
    "I want to start training immediately to compare the utilizations.\n",
    "I wish I could set env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 so that it does not spend a lot of time running performance tests to find the best convolution algorithm. Not an easy job if you do not go into details of dockers. I'm not even sure what it does with this autotune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
